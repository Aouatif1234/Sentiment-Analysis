{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries\n"
      ],
      "metadata": {
        "id": "UP7ET-JV-2uJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import demoji\n",
        "import csv\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "WMM6Q5xe-5_j"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading NLTK Data\n"
      ],
      "metadata": {
        "id": "fYh-slGo_K5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Y6FSh33oTZFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Loading Dataset"
      ],
      "metadata": {
        "id": "FzPyi1sYTb7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./train.csv')\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "WDD-0mPGTeJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing Functions"
      ],
      "metadata": {
        "id": "-g5Hj9zqTmTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('arabic')\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "def removeStopWords(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_sentence = [w for w in word_tokens if w not in stop_words]\n",
        "    return ' '.join(filtered_sentence)\n",
        "\n",
        "def NormalizeArabic(text):\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    return text\n",
        "\n",
        "def arabic_diacritics(text):\n",
        "    arabic_diacritics = re.compile(\"\"\" ّ|َ|ً|ُ|ٌ|ِ|ٍ|ْ|ـ\"\"\", re.VERBOSE)\n",
        "    return re.sub(arabic_diacritics, '', text)\n",
        "\n",
        "def removeNumbers(text):\n",
        "    return ''.join([i for i in text if not i.isdigit()])\n",
        "\n",
        "def stemming(text):\n",
        "    st = ISRIStemmer()\n",
        "    stemmed_words = [st.stem(w) for w in word_tokenize(text)]\n",
        "    return \" \".join(stemmed_words)\n",
        "\n",
        "def remove_english_characters(text):\n",
        "    return re.sub(r'[a-zA-Z]+', '', text)\n",
        "\n",
        "def process(text):\n",
        "    text = \" \".join(re.split(r'[؟،.!()]', text))  # Splitting on Arabic and punctuation characters\n",
        "    return \" \".join(text.split())  # Remove multiple spaces\n",
        "\n",
        "# Preprocessing Data\n",
        "def preprocess_data(df, column_name):\n",
        "    for index, row in df.iterrows():\n",
        "        row[column_name] = removeStopWords(row[column_name])\n",
        "        row[column_name] = NormalizeArabic(row[column_name])\n",
        "        row[column_name] = arabic_diacritics(row[column_name])\n",
        "        row[column_name] = removeNumbers(row[column_name])\n",
        "        row[column_name] = row[column_name].translate(translator)\n",
        "        row[column_name] = stemming(row[column_name])\n",
        "        row[column_name] = process(row[column_name])\n",
        "        row[column_name] = demoji.replace(row[column_name], \"\")\n",
        "        df.at[index, column_name] = row[column_name]\n",
        "\n",
        "preprocess_data(df, 'comment')"
      ],
      "metadata": {
        "id": "lIcq2zmQTsR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction\n"
      ],
      "metadata": {
        "id": "kIpHvn6sTzwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(sublinear_tf=True, encoding='utf-8', ngram_range=(1, 2))\n",
        "features = tfidf.fit_transform(df['comment']).toarray()\n",
        "labels = df['label']"
      ],
      "metadata": {
        "id": "ZijQlpKDT03Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training and Hyperparameter Tuning\n"
      ],
      "metadata": {
        "id": "Q-EofvqrT3a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_rf = {'n_estimators': [800, 1000], 'max_features': [1, 0.5, 0.2], 'random_state': [3, 4, 5]}\n",
        "grid_search_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5)\n",
        "grid_search_rf.fit(features, labels)\n",
        "\n",
        "print(\"Final Score: \", round(grid_search_rf.score(features, labels) * 100, 4), \"%\")\n",
        "print(\"Best Parameters: \", grid_search_rf.best_params_)\n",
        "print(\"Best Estimator: \", grid_search_rf.best_estimator_)\n"
      ],
      "metadata": {
        "id": "vGp8zn5zT5lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Testing"
      ],
      "metadata": {
        "id": "1eewQ942T-24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('./test.csv')\n",
        "df_unseen = pd.DataFrame(data=df_test['comment'])\n",
        "\n",
        "preprocess_data(df_unseen, 'comment')\n",
        "test_features = tfidf.transform(df_unseen['comment']).toarray()\n",
        "y_pred = grid_search_rf.predict(test_features)\n",
        "\n",
        "# Saving Predictions to CSV\n",
        "csv_columns = ['id', 'label']\n",
        "csv_file = \"prediction.csv\"\n",
        "\n",
        "try:\n",
        "    with open(csv_file, 'w', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
        "        writer.writeheader()\n",
        "        for i in range(df_test.shape[0]):\n",
        "            data = {'id': i + 1, 'label': y_pred[i]}\n",
        "            writer.writerow(data)\n",
        "except IOError:\n",
        "    print(\"I/O error\")"
      ],
      "metadata": {
        "id": "I6rF47nbULW9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}